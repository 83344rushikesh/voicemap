{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE, MDS\n",
    "from keras.models import load_model\n",
    "from IPython.display import SVG, Audio, display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (utils.py, line 73)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"../utils.py\"\u001b[0;36m, line \u001b[0;32m73\u001b[0m\n\u001b[0;31m    encoder = Model(inputs=, outputs=model.layers[-2](inputs))\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from data import LibriSpeechDataset\n",
    "from utils import whiten, contrastive_loss\n",
    "from config import LIBRISPEECH_SAMPLING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/convnet_contrastive_loss.hdf5'\n",
    "downsampling = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese = load_model(model_path, custom_objects=[con])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(siamese, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(siamese.layers[2], show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(12000,1))\n",
    "\n",
    "encoded = siamese.layers[2](inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=inputs, outputs=encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = siamese.layers[2]\n",
    "encoder.compile(loss='mse',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(encoder, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(downsampling, whitening=True):\n",
    "    def preprocessor_(batch):\n",
    "        ([i_1, i_2], labels) = batch\n",
    "        i_1 = i_1[:, ::downsampling, :]\n",
    "        i_2 = i_2[:, ::downsampling, :]\n",
    "        if whitening:\n",
    "            i_1, i_2 = whiten(i_1), whiten(i_2)\n",
    "\n",
    "        return [i_1, i_2], labels\n",
    "\n",
    "    return preprocessor_\n",
    "\n",
    "\n",
    "whiten_downsample = preprocessor(downsampling, whitening=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = ['train-clean-100', 'train-clean-360']\n",
    "train = LibriSpeechDataset(training_set, 3, stochastic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = [train[i] for i in np.random.randint(0,len(train),size=n_samples)]\n",
    "X = np.stack(zip(*Z)[0])[:, :, np.newaxis]\n",
    "y = np.stack(zip(*Z)[1])[:, np.newaxis]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X, _], _ = whiten_downsample(([X, X], []))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random samples from subset of speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_speakers = 20\n",
    "m_samples = 10\n",
    "n_random_speakers = train.df['speaker_id'].sample(n_speakers).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get m samples from each speaker\n",
    "X, y = [], []\n",
    "for i in n_random_speakers:\n",
    "    ids = train.df[train.df['speaker_id']==i]['id'].sample(m_samples).values\n",
    "    Z = [train[i] for i in ids]\n",
    "    X_ = np.stack(zip(*Z)[0])[:, :, np.newaxis]\n",
    "    y_ = np.stack(zip(*Z)[1])[:, np.newaxis]\n",
    "    [X_, _], _ = whiten_downsample(([X_, X_], []))\n",
    "    \n",
    "    X.append(X_)\n",
    "    y.append(ids)\n",
    "    \n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = encoder.predict(X)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_samples(a, b):\n",
    "    sample_a = train[a]\n",
    "    sample_b = train[b]\n",
    "\n",
    "    print 'Sample A ({}):'.format(a)\n",
    "    display(Audio(data=sample_a[0],rate=LIBRISPEECH_SAMPLING_RATE))\n",
    "    print 'Sample B ({}):'.format(b)\n",
    "    display(Audio(data=sample_b[0],rate=LIBRISPEECH_SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS()\n",
    "\n",
    "mds_embeddings = mds.fit_transform(embeddings)\n",
    "mds_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=30,learning_rate=500)\n",
    "\n",
    "tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "tsne_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_embeddings = tsne_embeddings\n",
    "# two_d_embeddings = mds_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gender_markers = np.array([0 if train.df[train.df['id']==i]['sex'].values[0] == 'M' else 1 for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.hstack([two_d_embeddings, y[:, np.newaxis], gender_markers[:, np.newaxis]])\n",
    "m = Z[Z[:, 3] == 0]\n",
    "f = Z[Z[:, 3] == 1]\n",
    "m.shape, f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.scatter(m[:, 0], m[:, 1], c=m[:, 2], marker='o')\n",
    "plt.scatter(f[:, 0], f[:, 1], c=f[:, 2], marker='x')\n",
    "\n",
    "# for x_, y_, idx in zip(two_d_embeddings[:, 0], two_d_embeddings[:, 1], y):\n",
    "#     plt.text(x_, y_, idx)\n",
    "    \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_samples(76518, 20765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
